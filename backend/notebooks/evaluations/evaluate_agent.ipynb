{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Azure Evaluation SDK\n",
    "\n",
    "Leverage the evaluation to test the generated output of you agent(s) to verfiy the accuracy, preformance, clarity, coherece, risk and safety and more. You can even build your own evaluators!\n",
    "\n",
    "To see all the avaliable metrics visit https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ðŸ¤™'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "api_url = \"http://localhost:8000\"   # FastAPI uvicorn URL with port 8000\n",
    "# api_url = \"http://localhost:80\"     # Docker container URL since we exposed the port 80\n",
    "# api_url = \"https://chinook-backend-api.azurewebsites.net\"  # Azure Web App URL\n",
    "# api_url = \"http://20.118.71.68:80\"  # AKS URL\n",
    "\n",
    "res = requests.get(f\"{api_url}/health\")\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\":\"ðŸ¤™\"}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "\n",
    "\n",
    "# AI assisted quality evaluator\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the semantic meaning of generated answer to true answer, GroundTruth and the Relevance of the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.evaluation import RelevanceEvaluator, SimilarityEvaluator\n",
    "\n",
    "\n",
    "\"\"\"The relevance measure assesses the ability of answers to capture the key points of the context.\n",
    "High relevance scores signify the AI system's understanding of the input and its capability to produce coherent and contextually appropriate outputs. \n",
    "Conversely, low relevance scores indicate that generated responses might be off-topic, lacking in context, or insufficient in addressing the user's intended queries. \n",
    "Use the relevance metric when evaluating the AI system's performance in understanding the input and generating contextually appropriate responses.\n",
    "Relevance scores range from 1 to 5, with 1 being the worst and 5 being the best.\"\"\"\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"The similarity measure evaluates the likeness between a ground truth sentence (or document) and the AI model's generated prediction. \n",
    "This calculation involves creating sentence-level embeddings for both the ground truth and the model's prediction, which are high-dimensional vector\n",
    " representations capturing the semantic meaning and context of the sentences.\n",
    "\n",
    "Use it when you want an objective evaluation of an AI model's performance, particularly in text generation tasks where you have access to ground truth responses.\n",
    " Similarity enables you to assess the generated text's semantic alignment with the desired content, helping to gauge the model's quality and accuracy.\n",
    "\n",
    "Similarity scores range from 1 to 5, with 1 being the least similar and 5 being the most similar.\"\"\"\n",
    "\n",
    "\n",
    "# Initialzing Similarity Evaluator\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(f\"{api_url}/sql-invoke\",\n",
    "    json={\n",
    "        \"message\": \"Find albums released by artists who have more than 5 albums\",\n",
    "        \"thread_id\": \"847c6285-8fc9-4560-a83f-4e6285809364\"\n",
    "    }\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Invoke Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_sql_query(message, thread_id):\n",
    "    try:        \n",
    "        res = requests.post(f\"{api_url}/sql-invoke\",\n",
    "            json={\n",
    "                \"message\": message,\n",
    "                \"thread_id\": thread_id\n",
    "            }\n",
    "        )\n",
    "        print(res.json()[\"content\"])\n",
    "        return res.json()[\"content\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Local Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = str(uuid.uuid4())\n",
    "results = invoke_sql_query(\"Find albums released by artists who have more than 5 albums\", thread_id)\n",
    "ground_truth =\"\"\"Here are some albums released by artists who have more than 5 albums:\n",
    "\n",
    "### **Deep Purple**\n",
    "1. Come Taste The Band\n",
    "2. Deep Purple In Rock\n",
    "3. Fireball\n",
    "4. Knocking at Your Back Door: The Best Of Deep Purple in the 80's\n",
    "5. MK III The Final Concerts [Disc 1]\n",
    "6. Machine Head\n",
    "7. Purpendicular\n",
    "8. Slaves And Masters\n",
    "9. Stormbringer\n",
    "10. The Battle Rages On\n",
    "11. The Final Concerts (Disc 2)\n",
    "\n",
    "### **Iron Maiden**\n",
    "1. A Matter of Life and Death\n",
    "2. A Real Dead One\n",
    "3. A Real Live One\n",
    "4. Brave New World\n",
    "5. Dance Of Death\n",
    "6. Fear Of The Dark\n",
    "7. Iron Maiden\n",
    "8. Killers\n",
    "9. Live After Death\n",
    "10. Live At Donington 1992 (Disc 1)\n",
    "11. Live At Donington 1992 (Disc 2)\n",
    "12. No Prayer For The Dying\n",
    "13. Piece Of Mind\n",
    "14. Powerslave\n",
    "15. Rock In Rio [CD1]\n",
    "16. Rock In Rio [CD2]\n",
    "17. Seventh Son of a Seventh Son\n",
    "18. Somewhere in Time\n",
    "19. The Number of The Beast\n",
    "20. The X Factor\n",
    "21. Virtual XI\n",
    "\n",
    "### **Led Zeppelin**\n",
    "1. BBC Sessions [Disc 1] [Live]\n",
    "2. BBC Sessions [Disc 2] [Live]\n",
    "3. Coda\n",
    "4. Houses Of The Holy\n",
    "5. IV\n",
    "6. In Through The Out Door\n",
    "7. Led Zeppelin I\n",
    "8. Led Zeppelin II\n",
    "9. Led Zeppelin III\n",
    "10. Physical Graffiti [Disc 1]\n",
    "11. Physical Graffiti [Disc 2]\n",
    "12. Presence\n",
    "13. The Song Remains The Same (Disc 1)\n",
    "14. The Song Remains The Same (Disc 2)\n",
    "\n",
    "### **Metallica**\n",
    "1. ...And Justice For All\n",
    "2. Black Album\n",
    "3. Garage Inc. (Disc 1)\n",
    "4. Garage Inc. (Disc 2)\"\"\"\n",
    "\n",
    "\n",
    "relevance_score = relevance_eval(\n",
    "    response= results,\n",
    "    context= ground_truth,\n",
    "    query=\"Find albums released by artists who have more than 5 albums\",\n",
    ")\n",
    "print(relevance_score)\n",
    "\n",
    "\n",
    "similarity_score = similarity_eval(\n",
    "    query=\"Find albums released by artists who have more than 5 albums\",\n",
    "    response=results,\n",
    "    ground_truth=ground_truth)\n",
    "\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Local Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "# Define the input and output file paths\n",
    "file_path_input = './data/question_batch.json'\n",
    "file_path_output = './data/question_batch_output.json'\n",
    "\n",
    "# Read input JSON file\n",
    "with open(file_path_input, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare output structure\n",
    "output_data = {\"Results\": []}\n",
    "\n",
    "# Process each question\n",
    "for q in data['Questions']:\n",
    "    thread_id = str(uuid.uuid4())  # Generate unique thread ID\n",
    "\n",
    "    # Extract question and ground truth\n",
    "    question = q[\"Question\"]\n",
    "    ground_truth = q[\"GroundTruth\"]\n",
    "\n",
    "    # Call the function to get a response \n",
    "    results = invoke_sql_query(question, thread_id)\n",
    "\n",
    "    # Compute relevance and similarity scores \n",
    "    relevance_score = relevance_eval(response=results, context=ground_truth, query=question)\n",
    "    similarity_score = similarity_eval(query=question, response=results, ground_truth=ground_truth)\n",
    "\n",
    "    # Store results\n",
    "    output_data[\"Results\"].append({\n",
    "        \"Question\": question,\n",
    "        \"Answer\": results,\n",
    "        \"GroundTruth\": ground_truth,\n",
    "        \"RelevanceScore\": relevance_score[\"relevance\"],\n",
    "        \"SimilarityScore\": similarity_score[\"similarity\"]\n",
    "    })\n",
    "\n",
    "    # Print scores for debugging\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {results}\")\n",
    "    print(f\"Relevance Score: {relevance_score}\")\n",
    "    print(f\"Similarity Score: {similarity_score}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Write results to the output JSON file\n",
    "with open(file_path_output, 'w', encoding='utf-8') as file_output:\n",
    "    json.dump(output_data, file_output, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to {file_path_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
